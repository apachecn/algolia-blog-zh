# é«˜æ€§èƒ½æ¨èç³»ç»Ÿå‰–æ-ç¬¬ä¸‰éƒ¨åˆ†- Algolia åšå®¢

> åŸæ–‡ï¼š<https://www.algolia.com/blog/ai/the-anatomy-of-high-performance-recommender-systems-part-3/>

# è¿›å…¥æ¨èç³»ç»Ÿçš„å·¥ç¨‹ç‰¹å¾

åœ¨æœ¬ç³»åˆ— çš„ç¬¬ä¸€ç¯‡ [ä¸­ï¼Œæˆ‘ä»¬è°ˆåˆ°äº†ä¸€ä¸ªé«˜æ€§èƒ½æ¨èç³»ç»Ÿçš„å…³é”®ç»„ä»¶:(1)](https://www.algolia.com/blog/ai/the-anatomy-of-high-performance-recommender-systems-part-1/) [æ•°æ®æºã€](https://www.algolia.com/blog/ai/the-anatomy-of-high-performance-recommender-systems-part-2/) (2) **ç‰¹å¾åº“**ã€(3) [æœºå™¨å­¦ä¹ æ¨¡å‹](https://www.algolia.com/blog/ai/the-anatomy-of-high-performance-recommender-systems-part-iv/)ã€(4 & 5) [é¢„æµ‹&è¡ŒåŠ¨](https://www.algolia.com/blog/ai/the-anatomy-of-high-performance-recommender-systems-part-v/)ã€(6)ç»“æœã€(7)è¯„ä¼°å’Œ(8) AI ä¼¦ç†ã€‚

åœ¨è¿™ç¬¬ä¸‰ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨æ¨èç³»ç»Ÿçš„ç‰¹å¾å·¥ç¨‹è¿™ä¸€ä¸»é¢˜ã€‚è™½ç„¶ä»åŸå§‹æ•°æ®åˆ°æ¨èçš„è·¯å¾„è¦ç»è¿‡å„ç§å·¥å…·å’Œç³»ç»Ÿï¼Œä½†è¯¥è¿‡ç¨‹æ¶‰åŠä¸¤ä¸ªæ•°å­¦å®ä½“ï¼Œå®ƒä»¬æ˜¯ä»»ä½•æ¨èç³»ç»Ÿçš„é¢åŒ…å’Œé»„æ²¹: **ç‰¹å¾** å’Œ **æ¨¡å‹ã€‚**

![image from data to recommendations](img/3f60fea7ca1c66bab6b7c07b1d84380b.png)

ç‰¹å¾æ˜¯åŸå§‹æ•°æ®çš„æ•°å­—è¡¨ç¤ºã€‚ç‰¹å¾å·¥ç¨‹æ˜¯åœ¨ç»™å®šæ•°æ®ã€æ¨¡å‹å’Œä»»åŠ¡çš„æƒ…å†µä¸‹ç»„åˆæœ€åˆé€‚çš„ç‰¹å¾çš„è¿‡ç¨‹ã€‚åœ¨åŸºæœ¬çš„ååŒè¿‡æ»¤åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ²¡æœ‰ç‰¹å¾ï¼Œå› ä¸ºè¯„çº§å®é™…ä¸Šæ˜¯æ ‡ç­¾ã€‚

åŸºäºå†…å®¹çš„ç³»ç»Ÿå¤„ç†å„ç§å„æ ·çš„ç‰©å“æè¿°å’Œå…³äºç”¨æˆ·çš„çŸ¥è¯†ã€‚ç‰¹å¾å·¥ç¨‹åŒ…æ‹¬å°†è¿™äº›ä¸åŒç±»å‹çš„éç»“æ„åŒ–æ•°æ®è½¬æ¢æˆæ ‡å‡†åŒ–çš„æè¿°ã€‚å°½ç®¡å¯ä»¥ä½¿ç”¨ä»»ä½•ç±»å‹çš„è¡¨ç¤ºï¼Œæ¯”å¦‚å¤šç»´æ•°æ®è¡¨ç¤ºï¼Œä½†æœ€å¸¸è§çš„æ–¹æ³•æ˜¯ä»åº•å±‚æ•°æ®ä¸­æå–å…³é”®å­—ã€‚

é¡¹ç›®æœ‰å¤šä¸ªå­—æ®µï¼Œå…¶ä¸­åˆ—å‡ºäº†å„ç§å±æ€§ã€‚ä¾‹å¦‚ï¼Œä¹¦ç±æœ‰æè¿°ã€æ ‡é¢˜å’Œä½œè€…ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™äº›æè¿°å’Œå±æ€§å¯ä»¥è½¬æ¢æˆå…³é”®å­—ã€‚

| **ItemID** | **æ ‡é¢˜** | **ä½œè€…** | **æè¿°** | **æµæ´¾** | **ä»·æ ¼** |
| 0000031852 | 2034:ä¸‹ä¸€æ¬¡ä¸–ç•Œå¤§æˆ˜çš„å°è¯´ | ç¾å›½æµ·å†›ä¸Šå°†è©¹å§†æ–¯Â·æ–¯å¡”å¤«é‡Œè¿ªæ–¯Â·é˜¿å…‹æ›¼å· | æ¥è‡ªä¸¤ä½å‰å†›å®˜å’Œè·å¥–ä½œå®¶ï¼Œè¿™æ˜¯ä¸€éƒ¨ä»¤äººæ¯›éª¨æ‚šç„¶çš„çœŸå®åœ°ç¼˜æ”¿æ²»æƒŠæ‚šç‰‡ï¼Œæƒ³è±¡äº† 2034 å¹´ç¾å›½å’Œä¸­å›½åœ¨å—ä¸­å›½æµ·çš„æµ·å†›å†²çªï¼Œä»¥åŠä»é‚£é‡Œé€šå¾€å™©æ¢¦èˆ¬çš„å…¨çƒå¤§ç¾éš¾çš„é“è·¯ã€‚ | æƒŠæ‚šç‰‡&æ‚¬ç–‘ç‰‡ | 17.84 ç¾å…ƒ |

å¦ä¸€æ–¹é¢ï¼Œå½“å±æ€§åŒ…å«æ•°å­—é‡(å¦‚ä»·æ ¼)æˆ–ä»å¯èƒ½æ€§çš„å°å®‡å®™(å¦‚ç±»å‹)ä¸­æå–çš„å­—æ®µæ—¶ï¼Œæ‚¨å¯ä»¥ç›´æ¥ä½¿ç”¨å¤šç»´(ç»“æ„åŒ–)è¡¨ç¤ºã€‚

é™¤äº†æè¿°å•†å“ï¼Œä½ è¿˜éœ€è¦æ”¶é›†ç”¨æˆ·èµ„æ–™ã€‚æ¯”å¦‚[movie lens](https://grouplens.org/datasets/movielens/1m/)ï¼Œä¸€ä¸ªç»å…¸çš„ç”¨æˆ·å±æ€§æ¨èæ•°æ®é›†ï¼Œ åŒ…å«äº†æ€§åˆ«ã€å¹´é¾„ã€èŒä¸šä¸‰ä¸ªç”¨æˆ·å±æ€§ã€‚å› ä¸ºè¿™äº›æ˜¯å•æ ‡ç­¾å±æ€§ï¼Œæ‰€ä»¥å¯ä»¥åœ¨é¢„å¤„ç†è¿‡ç¨‹ä¸­ä½¿ç”¨ [ä¸€é”®](https://en.wikipedia.org/wiki/One-hot) ç¼–ç å¯¹å®ƒä»¬è¿›è¡Œç¼–ç ã€‚

| **ç”¨æˆ·æ ‡è¯†** | **æ€§åˆ«** | **å¹´é¾„** | **èŒä¸š** |
|  | ç”·/å¥³ | * 1:â€œ18 å²ä»¥ä¸‹â€  * 18:â€œ18-24 å²â€  * 25:â€œ25-34 å²â€  * 35:â€œ35-44 å²â€  * 45:â€œ45 | * 0:â€œå…¶ä»–â€æˆ–æœªæŒ‡å®š
* 1:â€œå­¦æœ¯/æ•™è‚²å·¥ä½œè€…â€
* 2:â€œè‰ºæœ¯å®¶â€
* 3:â€œæ–‡ä¹¦/è¡Œæ”¿â€
* 4:â€œå­¦é™¢/ç ”ç©¶ç”Ÿâ€
* 5:â€œå®¢æœâ€
* 6:â€œåŒ»ç”Ÿ/ä¿å¥â€ 12:â€œç¨‹åºå‘˜â€
* 13:â€œé€€ä¼‘â€
* 14:â€œé”€å”®/è¥é”€â€
* 15:â€œç§‘å­¦å®¶â€
* 16:â€œä¸ªä½“æˆ·â€
* 17:â€œæŠ€æœ¯å‘˜/å·¥ç¨‹å¸ˆâ€
* 18:â€œå·¥åŒ â€ |

æœ€åï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰çš„ç‰¹å¾éƒ½æ˜¯å¹³ç­‰çš„ã€‚æ‚¨å¯ä»¥åº”ç”¨ç‰¹æ€§ **æƒé‡ã€** æ ¹æ®ç‰¹æ€§çš„é‡è¦æ€§ç»™å‡ºä¸åŒçš„æƒé‡ï¼Œæˆ–è€…åº”ç”¨ç‰¹æ€§ **é€‰æ‹©ã€** æ ¹æ®ç›¸å…³æ€§åŒ…æ‹¬æˆ–æ’é™¤å±æ€§ã€‚

ç°åœ¨è®©æˆ‘ä»¬æ¢ç´¢æ¨èå¼•æ“ä¸­æœ€å¸¸è§çš„é¡¹ç›®å’Œç”¨æˆ·å±æ€§çš„ç‰¹å¾å·¥ç¨‹æ–¹æ³•ã€‚

## [](#numerical-features)æ•°å­—ç‰¹å¾

### [](#discretization)ç¦»æ•£åŒ–

é¡¹ç›®æ•°æ®é›†ä¸­åŒ…å«çš„ä»·æ ¼å±æ€§æ˜¯ä¸€ä¸ª **è¿ç»­å˜é‡** ï¼Œå› ä¸ºå®ƒå¯ä»¥å–ä¸å¯æ•°çš„ä¸€ç»„å€¼ï¼Œå¹¶ä¸”å¯ä»¥åŒ…å«ç»™å®šèŒƒå›´å†…çš„ä»»ä½•å€¼ã€‚ä¸ºäº†å°†è¿™ç§åŸå§‹ç‰¹å¾è½¬æ¢æˆæœºå™¨å­¦ä¹ æ¨¡å‹å¯ä»¥æ¥å—çš„æ ¼å¼ï¼Œæ‚¨å°†ä½¿ç”¨ **é‡åŒ–:** æœ¬è´¨ä¸Šï¼Œå°†è¿ç»­å€¼æ˜ å°„åˆ°ç¦»æ•£å€¼ã€‚ä»æ¦‚å¿µä¸Šè®²ï¼Œè¿™å¯ä»¥è¢«è®¤ä¸ºæ˜¯ [ç»Ÿè®¡ç®±](https://en.wikipedia.org/wiki/Data_binning) çš„æœ‰åºåºåˆ—ã€‚

![image of the discretization process](img/50c0b5674b199f4c61a5fa0b65d79e61.png)

å®æ»¨æœ‰ä¸‰ç§å…¸å‹çš„æ–¹æ³•:

*   **ç»Ÿä¸€:** æ‰€æœ‰ç®±æŸœå®½åº¦ç›¸åŒ
*   **åŸºäºåˆ†ä½æ•°:** æ‰€æœ‰ç®±éƒ½æœ‰ç›¸åŒæ•°é‡çš„å€¼
*   **åŸºäº K-å‡å€¼:** æ¯ä¸ªä»“å±äºæœ€è¿‘çš„ä¸€ç»´ K-å‡å€¼èšç±»

ç»Ÿä¸€å®æ»¨æ˜¯æœ€ç®€å•çš„æ–¹æ³•ã€‚å®ƒä½¿ç”¨å…¬å¼å°†ä¸€ç³»åˆ—å¯èƒ½çš„å€¼åˆ’åˆ†ä¸ºå®½åº¦ç›¸åŒçš„ *N* ä¸ªç®±

![\[width = \frac{maxvalue-minvalue}{N}\]](img/65e5d34736595027f681850c6fbce337.png "Rendered by QuickLaTeX.com")

å…¶ä¸­ *N* ä¸ºåŒºé—´æ•°ã€‚

*N* é€šå¸¸æ˜¯é€šè¿‡å®éªŒç¡®å®šçš„â€”â€”è¿™é‡Œæ²¡æœ‰ç»éªŒæ³•åˆ™ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœå˜é‡ interval ä¸º[10ï¼Œ110]ï¼Œå¹¶ä¸”æ‚¨æƒ³è¦åˆ›å»º 5 ä¸ªå®¹å™¨ï¼Œè¿™æ„å‘³ç€`110-10 / 5 = 20`ï¼Œå› æ­¤æ¯ä¸ªå®¹å™¨çš„å®½åº¦ä¸º 20ï¼Œé—´éš”ä¸º[10ï¼Œ30]ï¼Œ[30ï¼Œ50]ï¼Œ[50ï¼Œ70]ï¼Œ[70â€“90]å’Œ[90ï¼Œ110]ã€‚

ç»Ÿä¸€ã€åˆ†ä½æ•°æˆ– k å‡å€¼å®æ»¨çš„ä»£ç å’Œç›´æ–¹å›¾å¦‚ä¸‹:

```
from sklearn.preprocessing import KBinsDiscretizer
# create the discretizer object with strategy uniform
discretizer = KBinsDiscretizer(bins, encode='ordinal', strategy='uniform') # replace "uniform" with "quantile" or "kmeans" to change discretization strategies
data_disc= discretizer.fit_transform(data)
```

| **ç»Ÿä¸€(åƒåœ¾ç®±= 10)** | **åˆ†ä½æ•°(ä»“= 10)** | **K-å‡å€¼(ä»“= 10)** |
| ![image of bins](img/50d2e645a4e8d2b1bd80d15a1914787a.png) | ![image of bins](img/2991152ee4891b4be4546f15a7cadf89.png) | ![image of bins](img/a9892a752acff426d65a05abc98fc363.png) |

### [](#normalization-and-standardization)è§„èŒƒåŒ–å’Œæ ‡å‡†åŒ–

è®¨è®ºæœ€å¤šçš„ä¸¤ç§ç¼©æ”¾æ–¹æ³•æ˜¯**(å°†å€¼é‡æ–°ç¼©æ”¾åˆ°[0ï¼Œ1]çš„èŒƒå›´å†…)å’Œ **æ ‡å‡†åŒ–** (å°†æ•°æ®é‡æ–°ç¼©æ”¾åˆ°å¹³å‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1)ã€‚ä»¥ä¸‹æ˜¯æ•°æ®ç»è¿‡è§„èŒƒåŒ–å’Œæ ‡å‡†åŒ–åçš„å¯è§†åŒ–è¡¨ç¤º:**

 **![image of the difference-between-normalization-standardization](img/9c130aed8bd1042f8940c5019a25cd11.png)

æ‚¨å¯ä»¥å¯¹é¡µé¢æµè§ˆé‡ã€ç‚¹å‡»é‡å’Œäº¤æ˜“é‡ç­‰ç‰¹å¾ä½¿ç”¨å½’ä¸€åŒ–ï¼Œå› ä¸ºè¿™äº›å€¼ä¸æ˜¯æ­£æ€(é«˜æ–¯)åˆ†å¸ƒçš„ï¼Œå¤§å¤šæ•°æ—¶å€™å®ƒä»¬æ˜¯é•¿å°¾ã€‚

è¿™æ˜¯å½’ä¸€åŒ–çš„å…¬å¼:

![\[{X}' = \frac{X-X_{min} }{X_{max}-X_{min}}\]](img/32490b73ed2bbcbcf446b1229a8f358c.png "Rendered by QuickLaTeX.com")

å…¶ä¸­*Xmax*å’Œ*Xmin*åˆ†åˆ«ä¸ºç‰¹å¾çš„æœ€å¤§å€¼å’Œæœ€å°å€¼ã€‚

```
# data normalization with sklearn
from sklearn.preprocessing import MinMaxScaler

# fit scaler on training data
norm = MinMaxScaler().fit(X_train)

# transform training data
X_train_norm = norm.transform(X_train)

# transform testing data
X_test_norm = norm.transform(X_test)

```

æ ‡å‡†åŒ–å¯¹å®¢æˆ·è¯„è®ºå¾ˆæœ‰ç”¨ï¼Œå› ä¸ºæ•°æ®éµå¾ªé«˜æ–¯(æ­£æ€)åˆ†å¸ƒ:



å…¶ä¸­ *Î¼* æ˜¯ç‰¹å¾å€¼çš„å¹³å‡å€¼ï¼Œğˆæ˜¯ç‰¹å¾å€¼çš„æ ‡å‡†å·®ã€‚æ³¨æ„ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¿™äº›å€¼ä¸é™äºç‰¹å®šçš„èŒƒå›´ã€‚

```
from sklearn.preprocessing import StandardScaler

# fit on training data
scale = StandardScaler().fit(X_train)

# transform the training data 
X_train_stand = scale.transform(X_train)

# transform the testing data 
X_test_stand = scale.transform(X_test)

```

## [](#categorical-features)åˆ†ç±»ç‰¹å¾

é€šå¸¸ï¼Œç‰¹å¾è¢«è¡¨ç¤ºä¸ºåˆ†ç±»å€¼ï¼Œè€Œä¸æ˜¯è¿ç»­å€¼ã€‚åœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œç”¨æˆ·å¯ä»¥æ‹¥æœ‰è¯¸å¦‚æ€§åˆ«([ç”·æ€§ã€å¥³æ€§])ã€å¹´é¾„([18 å²ä»¥ä¸‹ã€18-24 å²ã€25-34 å²])å’ŒèŒä¸š([å…¶ä»–ã€å­¦æœ¯/æ•™è‚²å·¥ä½œè€…ã€è‰ºæœ¯å®¶ã€èŒå‘˜/ç®¡ç†äººå‘˜]ï¼Œâ€¦)ç­‰ç‰¹å¾ã€‚è¿™æ ·çš„ç‰¹å¾å¯ä»¥æœ‰æ•ˆåœ°ç¼–ç ä¸ºæ•´æ•°ï¼Œä¾‹å¦‚ï¼Œ[ç”·æ€§ï¼Œ18-24 å²ï¼Œæ–‡ä¹¦/ç®¡ç†]å¯ä»¥è¡¨ç¤ºä¸º[0ï¼Œ1ï¼Œ3]ï¼Œè€Œ[å¥³æ€§ï¼Œ25-34 å²ï¼Œå­¦æœ¯/æ•™è‚²å·¥ä½œè€…]å¯ä»¥è¡¨ç¤ºä¸º[1ï¼Œ2ï¼Œ1]ã€‚

æˆ‘ä»¬æœ‰å‡ ä¸ªå°†åˆ†ç±»ç‰¹å¾è½¬æ¢æˆæ•´æ•°çš„é€‰é¡¹:

*   ä½¿ç”¨æ™®é€šç¼–ç å™¨ã€‚ è¯¥ä¼°è®¡å™¨å°†æ¯ä¸ªåˆ†ç±»ç‰¹å¾è½¬æ¢ä¸ºä¸€ä¸ªæ–°çš„æ•´æ•°ç‰¹å¾(0 åˆ° n _ categoriesâ€“1)

```
from sklearn.preprocessing import OrdinalEncoder

user_data = [['male', '18-24', 'clerical/admin'], ['female', '25-34', 'academic/educator']]
encoder = OrdinalEncoder().fit(user_data)
encoder.transform([['female', '25-34', 'clerical/admin']])

# array([[0., 1., 1.]]

```

*   ä½¿ç”¨ scikit-learn ä¼°è®¡å™¨ã€‚è¿™ä¸ªä¼°è®¡å™¨ä½¿ç”¨ä¸€ä¸ªâ€œK é€‰ä¸€â€çš„æ–¹æ¡ˆï¼Œä¹Ÿç§°ä¸ºä¸€é”®ç¼–ç æˆ–è™šæ‹Ÿç¼–ç ï¼Œå°†ç±»åˆ«è½¬æ¢ä¸ºæ•´æ•°ã€‚

```
from sklearn.preprocessing import OneHotEncoder

user_data = [['male', '18-24', 'clerical/admin'], ['female', '25-34', 'academic/educator']]
encoder = OneHotEncoder().fit(user_data)
encoder.transform([['female', '25-34', 'clerical/admin'],['male', '25-34', 'academic/educator']]).toarray()

# array([[1., 0., 0., 1., 0., 1.], [0., 1., 0., 1., 1., 0.]])

```

## [](#text-embedding)æ–‡å­—åµŒå…¥

è¿™äº›æ–¹æ³•â€”â€”æ ‡å‡†åŒ–ã€è§„èŒƒåŒ–å’Œåˆ†ç±»ç‰¹å¾â€”â€”ç”¨äºç»„åˆç‰¹å¾ã€‚å®ƒä»¬éƒ½ä¾èµ–äºå¯¹è¯­è¨€çš„è¯­ä¹‰ç†è§£ã€‚è®©æˆ‘ä»¬æ¥çœ‹çœ‹å¦‚ä½•é˜…è¯»åŸºäºæ–‡æœ¬çš„å†…å®¹ã€‚

è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œå¤„ç†äººç±»è¯­è¨€ã€‚æœ‰ä¸¤ç§æŠ€æœ¯å¯ä»¥å®Œæˆè¿™é¡¹ä»»åŠ¡:å¯¹æœªå¤„ç†çš„æ–‡æœ¬åº”ç”¨è¯è¢‹æ¨¡å‹ï¼Œå¹¶å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥ä¾¿ç¨åä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚

### [](#bag-of-words)è¢‹å­—

è¯æ±‡è¢‹æ¨¡å‹æ˜¯æœ€å¸¸ç”¨çš„æµç¨‹ï¼Œå› ä¸ºå®ƒæ˜“äºå®æ–½å’Œç†è§£ã€‚è¿™ä¸ªæƒ³æ³•æ˜¯ä¸ºå¥å­å’Œæ–‡æ¡£åˆ›å»ºä¸€ä¸ªå‡ºç°çŸ©é˜µï¼Œè€Œä¸è€ƒè™‘è¯­æ³•æˆ–è¯åºã€‚

```
from sklearn.feature_extraction.text import CountVectorizer

corpus = ["From two former military officers and award-winning authors, a chillingly authentic geopolitical thriller that imagines a naval clash between the US and China in the South China Sea in 2034â€“and the path from there to a nightmarish global conflagration."]

vectorizer = CountVectorizer(stop_words=None, ngram_range=(1, 1), min_df=1, max_df=1)
# stop_words - Please see the following guidelines before choosing a value for this param: https://scikit-learn.org/stable/modules/feature_extraction.html#stop-words
# ngram_range - Provides the range of n-values for the word n-grams to be extracted. (1, 1) means the vectorizer will only take into account unigrams, thus implementing the Bag-of-Words model
# min_df && max_df - The minimum and maximum document frequency required for a term to be included in the vocabulary

X = vectorizer.fit_transform(corpus)

print(vectorizer.get_feature_names())
'''
['2034', 'and', 'authentic', 'authors', 'award', 'between', 'chillingly', 'china', 'clash', 'conflagration', 'former', 'from', 'geopolitical', 'global', 'imagines', 'in', 'military', 'naval', 'nightmarish', 'officers', 'path', 'sea', 'south', 'that', 'the', 'there', 'thriller', 'to', 'two', 'us', 'winning']
'''

print(X.toarray())
#[[1 3 1 1 1 1 1 2 1 1 1 2 1 1 1 2 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1]]

```

äº§ç”Ÿçš„é¢‘ç‡ç”¨äºè®­ç»ƒåˆ†ç±»å™¨ã€‚è¯·æ³¨æ„ï¼Œå› ä¸ºä¸éœ€è¦å¯¹å¥å­è¿›è¡Œé¢„å¤„ç†ï¼Œæ‰€ä»¥è¿™ç§æ–¹æ³•ä¼šå¸¦æ¥ä¸€ç³»åˆ—ç¼ºç‚¹ï¼Œæ¯”å¦‚ç»“æœå‘é‡çš„ç¨€ç–è¡¨ç¤ºã€ç†è§£æ–‡æœ¬æ•°æ®çš„èƒ½åŠ›å·®ï¼Œä»¥åŠåœ¨å¤„ç†å¤§é‡æ–‡æ¡£æ—¶æ€§èƒ½ä¸ä½³ã€‚

### [](#preprocessing-text)é¢„å¤„ç†æ–‡æœ¬

é¢„å¤„ç†å¥å­çš„æ ‡å‡†é¡ºåºæ˜¯è®°å·åŒ–ã€å»æ‰ä¸å¿…è¦çš„æ ‡ç‚¹å’Œåœç”¨è¯ã€è¯å¹²åŒ–ã€è¯æ¡åŒ–ã€‚

æ ‡è®°åŒ–åŒ…æ‹¬æŠŠå¥å­å˜æˆå•è¯ã€‚æ¥è‡ª`nltk package`çš„`word_tokenizer`å¯¹å­—ç¬¦ä¸²è¿›è¡Œæ ‡è®°ï¼Œä»¥åˆ†ç¦»é™¤å¥ç‚¹ä»¥å¤–çš„æ ‡ç‚¹ç¬¦å·ã€‚

```
import nltk
nltk.download('punkt') # a pre-trained Punkt tokenizer for English
from nltk.tokenize import word_tokenize

text = "From two former military officers and award-winning authors, a chillingly authentic geopolitical thriller that imagines a naval clash between the US and China in the South China Sea in 2034â€“and the path from there to a nightmarish global conflagration."

tokens = word_tokenize(text)
print(tokens)
'''
['From', 'two', 'former', 'military', 'officers', 'and', 'award-winning', 'authors', ',', 'a', 'chillingly', 'authentic', 'geopolitical', 'thriller', 'that', 'imagines', 'a', 'naval', 'clash', 'between', 'the', 'US', 'and', 'China', 'in', 'the', 'South', 'China', 'Sea', 'in', '2034â€“and', 'the', 'path', 'from', 'there', 'to', 'a', 'nightmarish', 'global', 'conflagration', '.']
''' 

```

#### ä¸åŒçš„åŒ…éƒ½æœ‰é¢„å®šä¹‰çš„åœç”¨è¯ã€‚å¦ä¸€ç§æ–¹æ³•æ˜¯å®šä¹‰ä¸€ä¸ªä¸è¯­æ–™åº“ç›¸å…³çš„è‡ªå®šä¹‰åœç”¨è¯åˆ—è¡¨ã€‚

```
nltk.download('stopwords') # 2,400 stopwords for 11 languages
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
tokens = [w for w in tokens if not w in stop_words]

print(tokens)
'''
['From', 'two', 'former', 'military', 'officers', 'award-winning', 'authors', ',', 'chillingly', 'authentic', 'geopolitical', 'thriller', 'imagines', 'naval', 'clash', 'US', 'China', 'South', 'China', 'Sea', '2034â€“and', 'path', 'nightmarish', 'global', 'conflagration', '.']
'''

```

é€šè¿‡å»é™¤åç¼€å’Œå‰ç¼€ï¼Œå°†è¯­æ–™åº“ä¸­çš„å•è¯ç¼©å‡åˆ°å®ƒä»¬çš„æ ¹ã€‚è¯å¹²åˆ†æå™¨æŸ¥æ‰¾å¸¸è§åç¼€å’Œå‰ç¼€çš„åˆ—è¡¨ï¼Œå¹¶åˆ é™¤å®ƒä»¬ã€‚

```
from nltk.stem.porter import PorterStemmer #there are more available

porter = PorterStemmer()
stems = []
for t in tokens:    
    stems.append(porter.stem(t))

print(stems)
'''
['from', 'two', 'former', 'militari', 'offic', 'award-win', 'author', ',', 'chillingli', 'authent', 'geopolit', 'thriller', 'imagin', 'naval', 'clash', 'us', 'china', 'south', 'china', 'sea', '2034â€“and', 'path', 'nightmarish', 'global', 'conflagr', '.']
'''

```

è¯æ±‡åŒ–ä¸è¯å¹²åŒ–æœ‰ç›¸åŒçš„é¢„æœŸè¾“å‡º:å°†å•è¯ç®€åŒ–ä¸ºä¸€ä¸ªå…¬å…±åŸºæˆ–å…¶è¯æ ¹ã€‚ç„¶è€Œï¼Œlemmatizer è€ƒè™‘åˆ°äº†å•è¯çš„å½¢æ€åˆ†æï¼Œå¯¹æ‰€æœ‰è¯å½¢å˜åŒ–ä½¿ç”¨ç›¸åŒçš„è¯æ ¹ã€‚

```
nltk.download('wordnet') # a dictionary is needed for a Lemmatizer
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()
lemmas = []
for t in tokens:    
    lemmas.append(lemmatizer.lemmatize(t))
print(lemmas)

'''
['From', 'two', 'former', 'military', 'officer', 'award-winning', 'author', ',', 'chillingly', 'authentic', 'geopolitical', 'thriller', 'imago', 'naval', 'clash', 'US', 'China', 'South', 'China', 'Sea', '2034â€“and', 'path', 'nightmarish', 'global', 'conflagration', '.']
'''

```

## [](#image-encoding)å›¾åƒç¼–ç 

åœ¨è¿›å…¥è¿™ä¸ªè¯é¢˜ä¹‹å‰ï¼Œä½ å¿…é¡»äº†è§£è®¡ç®—æœºâ€œçœ‹â€å›¾åƒçš„æ–¹å¼ã€‚æ¯ä¸ªå›¾åƒè¢«è¡¨ç¤ºä¸º 1 æˆ– 3 ä¸ªåƒç´ çŸ©é˜µã€‚æ¯ä¸ªçŸ©é˜µä»£è¡¨ä¸€ä¸ª **é€šé“ã€‚** å¯¹äºé»‘ç™½å›¾åƒï¼Œåªæœ‰ä¸€ä¸ªé€šé“ï¼Œè€Œå¯¹äºå½©è‰²å›¾åƒï¼Œæœ‰ä¸‰ä¸ª:çº¢ã€ç»¿ã€è“ã€‚æ¯ä¸ªåƒç´ ä¾æ¬¡ç”± 0 åˆ° 255 ä¹‹é—´çš„æ•°å­—è¡¨ç¤ºï¼Œè¡¨ç¤ºé¢œè‰²çš„å¼ºåº¦ã€‚

![image of black and white](img/d33de02c46263dec66d8b62ac48afefc.png)

### [](#pixel-values-as-features)åƒç´ å€¼ä½œä¸ºç‰¹å¾

ä»å›¾åƒä¸­æ£€ç´¢ç‰¹å¾æœ€ç®€å•çš„æ–¹æ³•æ˜¯é‡æ–°æ’åˆ—æ‰€æœ‰åƒç´ ï¼Œç”Ÿæˆä¸€ä¸ªç‰¹å¾å‘é‡ã€‚å¯¹äºç°åº¦å›¾åƒï¼Œè¿™å¯ä»¥ä½¿ç”¨ NumPy: è½»æ¾å®ç°

```
import skimage
from skimage import data, io # data has standard test images
import numpy as np
from matplotlib import pyplot as plt
%matplotlib inline

camera = data.camera() 
io.imshow(camera)
plt.show()

print(camera.shape) # (height, width)
# (512, 512)
features = np.reshape(camera, (512*512))
print(features.shape, features)
# ((262144,), array([156, 157, 160, ..., 121, 113, 111], dtype=uint8))

```

ç›¸åŒçš„æŠ€æœ¯å¯ç”¨äº RGB å›¾åƒã€‚ç„¶è€Œï¼Œæ›´åˆé€‚çš„æ–¹æ³•æ˜¯é€šè¿‡ä½¿ç”¨æ¥è‡ªæ‰€æœ‰é€šé“çš„åƒç´ çš„å¹³å‡å€¼æ¥åˆ›å»ºç‰¹å¾å‘é‡ã€‚

```
import skimage
from skimage import data, io # data has standard test images
import numpy as np
from matplotlib import pyplot as plt
%matplotlib inline

astronaut = data.astronaut() 
io.imshow(astronaut)
plt.show()

print(astronaut.shape) # (height, width, no. of channels)
# (512, 512, 3)
feature_matrix = np.zeros((512, 512)) # matrix initialized with 0
for i in range(0, astronaut.shape[0]):
    for j in range(0, astronaut.shape[1]):
        feature_matrix[i][j] = ((astronaut[i, j, 0]) +(astronaut[i, j, 1]) + (astronaut[i, j, 2])) / 3

print(feature_matrix)
'''
array([[65.33333333, 26.66666667, 74.33333333, ..., 35.33333333,
        29\.        , 32.66666667],
       [ 2.33333333, 57.33333333, 31.66666667, ..., 33.66666667,
        30.33333333, 28.66666667],
       [25.33333333,  7.66666667, 80.33333333, ..., 36.33333333,
        32.66666667, 30.33333333],
       ...,
       [ 6.66666667,  7\.        ,  3\.        , ...,  0\.        ,
         0.33333333,  0\.        ],
       [ 3.33333333,  2.66666667,  4.33333333, ...,  0.33333333,
         1\.        ,  0\.        ],
       [ 3.66666667,  1.66666667,  0.33333333, ...,  0\.        ,
         1\.        ,  0\.        ]])
'''

```

### [](#edge-detection)è¾¹ç¼˜æ£€æµ‹

è¾¹ç¼˜æ˜¯å›¾åƒä¸­äº®åº¦å’Œé¢œè‰²æ€¥å‰§å˜åŒ–çš„ä¸€ç»„ç‚¹ã€‚æœ‰ä¸åŒçš„è¾¹ç¼˜æ£€æµ‹æŠ€æœ¯ï¼Œæœ€å¸¸ç”¨çš„æ˜¯ Canny è¾¹ç¼˜æ£€æµ‹ç®—æ³•ã€‚ä¸‹é¢æ˜¯å®ƒå¦‚ä½•å·¥ä½œçš„æ¦‚è¿°:

1.  ä½¿ç”¨é«˜æ–¯æ»¤æ³¢å™¨é™å™ª
2.  æ¢¯åº¦è®¡ç®—
3.  éæœ€å¤§å€¼æŠ‘åˆ¶(è¯¥ç®—æ³•éå†æ¢¯åº¦å¼ºåº¦çŸ©é˜µä¸Šçš„æ‰€æœ‰ç‚¹ï¼Œå¹¶æ‰¾åˆ°è¾¹ç¼˜æ–¹å‘ä¸Šå…·æœ‰æœ€å¤§å€¼çš„åƒç´ )
4.  åŒé˜ˆå€¼(å°†åƒç´ åˆ†ä¸ºä¸¤ç±»:å¼ºå’Œå¼±)
5.  æ»åè¾¹ç¼˜è·Ÿè¸ª(å½“ä¸”ä»…å½“æœ‰å¦ä¸€ä¸ªå¼ºåƒç´ ä½œä¸ºå…¶é‚»å±…æ—¶ï¼Œå°†å¼±åƒç´ è½¬æ¢ä¸ºå¼ºåƒç´ )

```
import skimage
from skimage import data # data has standard test images
import cv2
from matplotlib import pyplot as plt
%matplotlib inline

camera = data.camera() 
# from skimage import color # the image should be grayscale for Canny
# camera = color.rgb2gray(camera) # however this one already is
edges = cv2.Canny(camera,100,200) # thresholds for the hysteresis procedure

plt.subplot(121), plt.imshow(camera, cmap = 'gray')
plt.title('Original Image')
plt.subplot(122), plt.imshow(edges, cmap = 'gray')
plt.title('Edge Image')
plt.show()

```

## [](#final-word-and-feature-stores)

ä¸åŒçš„é¡¹ç›®å’Œå›¢é˜Ÿç»´æŠ¤å’Œæä¾›ç‰¹æ€§çš„æ–¹å¼å¯èƒ½ä¼šæœ‰å¾ˆå¤§çš„ä¸åŒã€‚è¿™å¢åŠ äº†åŸºç¡€æ¶æ„çš„å¤æ‚æ€§ï¼Œå¹¶ç»å¸¸å¯¼è‡´é‡å¤å·¥ä½œã€‚åˆ†å¸ƒå¼ç»„ç»‡é¢ä¸´çš„ä¸€äº›æŒ‘æˆ˜åŒ…æ‹¬:

*   ç‰¹å¾æœªè¢«é‡ç”¨
*   ç‰¹æ€§å®šä¹‰ä¸åŒ
*   è®¡ç®—ç‰¹å¾éœ€è¦å¾ˆé•¿æ—¶é—´
*   è®­ç»ƒå’Œå‘çƒä¸ä¸€è‡´
*   ç‰¹å¾è¡°å‡æœªçŸ¥

ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼Œä¸€ä¸ª **ç‰¹å¾åº“** ä½œä¸ºä¸€ä¸ªä¸­å¿ƒåº“ï¼Œç”¨äºå­˜å‚¨ä¸€ä¸ªç»„ç»‡å†…çš„æ–‡æ¡£åŒ–çš„ã€ç®¡ç†çš„å’Œè®¿é—®å—æ§çš„ç‰¹å¾ã€‚

| **ç‰¹å¾å­˜å‚¨** |
| **åç§°** | **æè¿°** | **å…ƒæ•°æ®** | **å®šä¹‰** |
| *å¹³å‡ _ ç”¨æˆ· _ è®¢å• _ ä»·å€¼* | *ä¸€ä¸ªç”¨æˆ·çš„å¹³å‡è®¢å•å€¼ã€‚* | ä¸ºä»€ä¹ˆå°†ç‰¹å¾æ·»åŠ åˆ°æ¨¡å‹ä¸­ï¼Œå®ƒå¦‚ä½•æœ‰åŠ©äºæ¦‚åŒ–ï¼Œè´Ÿè´£ç»´æŠ¤ç‰¹å¾æ•°æ®æºçš„ç»„ç»‡ä¸­çš„å·¥ç¨‹å¸ˆå§“åï¼Œè¾“å…¥ç±»å‹ï¼Œè¾“å‡ºç±»å‹ã€‚ | åœ¨è¿è¡Œæ—¶ç¯å¢ƒä¸­æ‰§è¡Œå¹¶åº”ç”¨äºè¾“å…¥ä»¥è®¡ç®—ç‰¹å¾å€¼çš„ç‰ˆæœ¬åŒ–ä»£ç ã€‚ |

æœ¬è´¨ä¸Šï¼Œç‰¹å¾åº“å…è®¸æ•°æ®å·¥ç¨‹å¸ˆæ’å…¥ç‰¹å¾ã€‚åè¿‡æ¥ï¼Œæ•°æ®åˆ†æå¸ˆå’Œæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆä½¿ç”¨ API æ¥è·å–ä»–ä»¬è®¤ä¸ºç›¸å…³çš„ç‰¹å¾å€¼ã€‚

æ­¤å¤–ï¼Œåº”å¯¹è¦ç´ å­˜å‚¨ä¸­çš„è¦ç´ å€¼è¿›è¡Œç‰ˆæœ¬åŒ–ï¼Œä»¥ç¡®ä¿æ•°æ®åˆ†æå¸ˆèƒ½å¤Ÿä½¿ç”¨ä¸ç”¨äºè®­ç»ƒå…ˆå‰æ¨¡å‹ç‰ˆæœ¬çš„è¦ç´ å€¼ç›¸åŒçš„è¦ç´ å€¼æ¥é‡æ–°æ„å»ºæ¨¡å‹ã€‚ç»™å®šè¾“å…¥çš„ç‰¹å¾å€¼æ›´æ–°åï¼Œå…ˆå‰çš„å€¼ä¸ä¼šè¢«åˆ é™¤ï¼›ç›¸åï¼Œå®ƒæ˜¯ç”¨æ—¶é—´æˆ³ä¿å­˜çš„ï¼Œè¡¨æ˜å®ƒæ˜¯ä½•æ—¶ç”Ÿæˆçš„ã€‚

åœ¨è¿‡å»çš„å‡ å¹´é‡Œï¼Œåˆ†æå¸ˆå’Œå·¥ç¨‹å¸ˆå‘æ˜ã€è¯•éªŒå¹¶éªŒè¯äº†å„ç§é€‚ç”¨äºç‰¹æ€§å·¥ç¨‹çš„æœ€ä½³å®è·µã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è§„èŒƒåŒ–ã€æ ‡å‡†åŒ–å’Œåˆ†ç±»ç‰¹æ€§ã€‚å…¶ä»–å®è·µåŒ…æ‹¬ç”Ÿæˆç®€å•çš„ç‰¹æ€§ã€é‡ç”¨é—ç•™ç³»ç»Ÿã€åœ¨éœ€è¦æ—¶ä½¿ç”¨ id ä½œä¸ºç‰¹æ€§ã€å°½å¯èƒ½å‡å°‘åŸºæ•°ã€è°¨æ…ä½¿ç”¨è®¡æ•°ã€åœ¨å¿…è¦æ—¶è¿›è¡Œç‰¹æ€§é€‰æ‹©ã€ä»”ç»†æµ‹è¯•ä»£ç ã€ä¿æŒä»£ç ã€æ¨¡å‹å’Œæ•°æ®åŒæ­¥ã€éš”ç¦»ç‰¹æ€§æå–ä»£ç ã€å°†æ¨¡å‹å’Œç‰¹æ€§æå–å™¨åºåˆ—åŒ–åœ¨ä¸€èµ·ï¼Œä»¥åŠè®°å½•ç‰¹æ€§çš„å€¼ã€‚

åŠŸèƒ½å·¥ç¨‹æ˜¯ä¸€ä¸ªåˆ›é€ æ€§çš„è¿‡ç¨‹ï¼Œä½œä¸ºä¸€åæœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆï¼Œä½ æœ€æœ‰èµ„æ ¼ç¡®å®šå“ªäº›åŠŸèƒ½é€‚åˆä½ çš„æ¨èæ¨¡å‹ã€‚

åœ¨æœ¬ç³»åˆ—çš„ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºæ„å»ºååŒè¿‡æ»¤æ¨èæ¨¡å‹ï¼Œè¿™å°†æ˜¯ä¸€ä»¶è½»è€Œæ˜“ä¸¾çš„äº‹æƒ…ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»è§£å†³äº†ç‰¹å¾å·¥ç¨‹çš„é—®é¢˜ã€‚æ•¬è¯·æœŸå¾…ï¼è¿˜æœ‰å¦‚æœæœ‰ä»€ä¹ˆé—®é¢˜ï¼Œå¯ä»¥åœ¨[Twitter](https://twitter.com/cborodescu)ä¸Šé—®æˆ‘ã€‚**